{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "ep04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "daGWjDiHLetV"
      },
      "source": [
        "name1 = \"Daniela Gonzalez Favero\" \n",
        "name2 = \"Miguel Pereira Ostrowski\"\n",
        "\n",
        "honorPledge = \"I affirm that I have not given or received any unauthorized \" \\\n",
        "              \"help on this assignment, and that this work is my own.\\n\"\n",
        "\n",
        "\n",
        "print(\"\\nName: \", name1)\n",
        "print(\"\\nHonor pledge: \", honorPledge)\n",
        "\n",
        "print(\"\\nName: \", name2)\n",
        "print(\"\\nHonor pledge: \", honorPledge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5uRbVdWMREt"
      },
      "source": [
        "# MAC0460 / MAC5832 (2021)\n",
        "---\n",
        "\n",
        "## EP4\n",
        "\n",
        "### Objectives:\n",
        "The aim of this EP is to\n",
        "- Practice training of linear, neural networks, and SVMs classifiers using the scikit-learn library (https://scikit-learn.org/)\n",
        "- Practice model evaluation, comparison and selection\n",
        "- Produce a summary report on the performed experiments and main findings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YZ7BAOt9Leta"
      },
      "source": [
        "# All imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd43x36LNHw9"
      },
      "source": [
        "# 1. Dataset preparation\n",
        "## 1.1. Downloading data\n",
        "Reading the MNIST dataset using the `tensorflow.keras` library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Tf8UO4Letb"
      },
      "source": [
        "(X_train_ori, y_train_ori), (X_test_ori, y_test_ori) = mnist.load_data()\n",
        "\n",
        "print(X_train_ori.shape, y_train_ori.shape)\n",
        "print(X_test_ori.shape, y_test_ori.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltIuMQZVLetb"
      },
      "source": [
        "labels = [\"%s\"%i for i in range(10)]\n",
        "\n",
        "unique, counts = np.unique(y_train_ori, return_counts=True)\n",
        "uniquet, countst = np.unique(y_test_ori, return_counts=True)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(unique - 0.2, counts, 0.25, label='Train')\n",
        "rects2 = ax.bar(unique + 0.2, countst, 0.25, label='Test')\n",
        "ax.legend()\n",
        "ax.set_xticks(unique)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "plt.title('MNIST classes')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2KnbATeLetc"
      },
      "source": [
        "fig, ax = plt.subplots(2, 3, figsize = (9, 6))\n",
        "\n",
        "for i in range(6):\n",
        "    ax[i//3, i%3].imshow(X_train_ori[i], cmap='gray')\n",
        "    ax[i//3, i%3].axis('off')\n",
        "    ax[i//3, i%3].set_title(\"Class: %d\"%y_train_ori[i])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHTViEENqTOS"
      },
      "source": [
        "## 1.2. Preprocessing\n",
        "Reducing the image size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wn9ExOMLetd"
      },
      "source": [
        "# Reduce the image size to its half \n",
        "X_train = np.array([image[::2, 1::2] for image in X_train_ori])\n",
        "X_test  = np.array([image[::2, 1::2] for image in X_test_ori])\n",
        "\n",
        "y_train = y_train_ori\n",
        "y_test = y_test_ori\n",
        "\n",
        "y_train_padrao = y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNtkwQ_KLetd"
      },
      "source": [
        "fig, ax = plt.subplots(2, 3, figsize = (9, 6))\n",
        "\n",
        "for i in range(6):\n",
        "    ax[i//3, i%3].imshow(X_train[i], cmap='gray')\n",
        "    ax[i//3, i%3].axis('off')\n",
        "    ax[i//3, i%3].set_title(\"Class: %d\"%y_train_ori[i])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQhfQxXlp_5h"
      },
      "source": [
        "Normalizing the intensities to the interval $[0, 1]$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9EhwWosLete"
      },
      "source": [
        "X_train = (X_train/255.0).astype('float32').reshape((60000,14*14))\n",
        "X_test = (X_test/255.0).astype('float32').reshape((10000,14*14))\n",
        "\n",
        "print(X_train.dtype)\n",
        "print(X_test.dtype)\n",
        "\n",
        "print(\"\\nShape of X_train: \", X_train.shape)\n",
        "print(\"Shape of X_test: \", X_test.shape)\n",
        "\n",
        "print(\"\\nMinimum value in X_train:\", np.amin(X_train))\n",
        "print(\"Maximum value in X_train:\", np.amax(X_train))\n",
        "\n",
        "print(\"\\nMinimum value in X_test:\", np.amin(X_test))\n",
        "print(\"Maximum value in X_test:\", np.amax(X_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAXBYpmSq2os"
      },
      "source": [
        "## 1.3. Partitioning dataset\n",
        "Partitioning the original training set into 70% training ($D_{train}$) and 30% validation ($D_{val}$) sets, in a stratified way:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAL2P834UEqu"
      },
      "source": [
        "SEED = 512\n",
        "FRACTION = 30/100\n",
        "D_train, D_val, y_train, y_val = train_test_split(X_train, \n",
        "                                                  y_train, \n",
        "                                                  test_size=FRACTION, \n",
        "                                                  random_state=SEED\n",
        "                                                 )\n",
        "\n",
        "print(D_train.shape)\n",
        "print(D_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMBy_iANQPw"
      },
      "source": [
        "# 2. Training, evaluating and selecting models\n",
        "Now, we will use $D_{train}$ to train and select three models: a logistic regression model, a neural network model, and a SVM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjTQR_EsrkVf"
      },
      "source": [
        "\n",
        "## 2.1. Model selection method\n",
        "\n",
        "\n",
        "To test different values for hyperparameters and choose the most adequate model, we use the `scikit-learn` library's class `GridSearchCV`. Defining some candidates to hyperparameters, the class will print some metrics and show how we should select the models.\n",
        "\n",
        "After that, we use some metrics implemented in `scikit-learn` library in the `model_selection_metrics()` function, to conclude the choices we made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG6ZPY5GpM7Z"
      },
      "source": [
        "def model_selection_metrics(y, y_predict):\n",
        "    \"\"\"\n",
        "    Computes three different metrics to evaluate model performance: 'Mean \n",
        "    squared error', 'Classification report' (several metrics, such as precision,\n",
        "    recall, etc) and the 'Confusion matrix'\n",
        "    :param y: class labels\n",
        "    :type y: np.ndarray(shape=(N, ))\n",
        "    :param y_predict: predicted class labels\n",
        "    :type y_predict: np.ndarray(shape=(N, ))\n",
        "    \"\"\"  \n",
        "\n",
        "    print()\n",
        "    print(\"Mean squared error:\\n\", mean_squared_error(y, y_predict))\n",
        "\n",
        "    print()\n",
        "    print(\"Classification report:\\n\", classification_report(y, y_predict))\n",
        "\n",
        "    print()\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(y, y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If9Wd1DFempX"
      },
      "source": [
        "## 2.2. Logistic Regression\n",
        "Firstly, we use the `GridSearchCV` to find the best cadidates to parameters of the Logistic Regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLL6oBMjCExK"
      },
      "source": [
        "tuned_parameters = [{'C':            np.logspace(-3,3,7),\n",
        "                     'penalty':      ['l2'],\n",
        "                     'solver':       ['newton-cg', 'lbfgs', 'sag'],\n",
        "                     'random_state': [SEED],\n",
        "                     'max_iter':     [2000]},\n",
        "                     {'C':           np.logspace(-3,3,7),\n",
        "                     'penalty':      ['l1', 'l2'],\n",
        "                     'solver':       ['liblinear'],\n",
        "                     'random_state': [SEED],\n",
        "                     'max_iter':     [2000]}\n",
        "                     ]\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(\n",
        "        LogisticRegression(), tuned_parameters, scoring='%s_macro' % score\n",
        "    )\n",
        "    clf.fit(D_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJiNM0R7FXm_"
      },
      "source": [
        "Thus, we conclude that the best parameters in this model are:\n",
        "```\n",
        "C = 1e-3,\n",
        "penalty = 'l2',\n",
        "solver = 'lbfgs',\n",
        "max_iter: 5000\n",
        "```\n",
        "(The selection of `max_iter` took into account how many iterations were necessary so that the model would converge, and it was adjusted manually)\n",
        "\n",
        "Now, let's use the `scikit-learn` library implementation of Logistic Regression to train the dataset with the chosen parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNslUgI4a8x_"
      },
      "source": [
        "# training\n",
        "clf = LogisticRegression(random_state=SEED, max_iter=2000).fit(D_train, y_train)\n",
        "\n",
        "# predicting\n",
        "y_predict = clf.predict(D_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjXhyHNudIz"
      },
      "source": [
        "Printing performance metrics such as the *classifier score* and the *model selection method* that we defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WYDkbIreEVW"
      },
      "source": [
        "print()\n",
        "print(\"Classifier score:\\n\" , clf.score(D_train, y_train))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_train, y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejvRxUFOkEkc"
      },
      "source": [
        "## 2.3. Neural Network\n",
        "Now, we use the `GridSearchCV` to find the best candidates to parameters of the Multi-layer Perceptron:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG6odelaEWP2"
      },
      "source": [
        "tuned_parameters = [{'hidden_layer_sizes': np.arange(10, 15),\n",
        "                     'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
        "                     'solver':     ['lbfgs', 'sgd', 'adam'],\n",
        "                     'alpha':      np.logspace(-4, -1, 4),         ,\n",
        "                     'learning_rate_init': np.logspace(-3, -1, 3),\n",
        "                     'random_state': [SEED],\n",
        "                     'max_iter':     [500]},\n",
        "                     ]\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(\n",
        "        MLPClassifier(), tuned_parameters, scoring='%s_macro' % score\n",
        "    )\n",
        "    clf.fit(D_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKiTKALbHUps"
      },
      "source": [
        "Thus, we conclude that the best parameters in this model are:\n",
        "```\n",
        "\n",
        "max_iter: 500\n",
        "```\n",
        "(The selection of `max_iter` took into account how many iterations were necessary so that the model would converge, and it was adjusted manually. We also observed that too many iterations would lead the model to overfit: it'd have `score=1`)\n",
        "\n",
        "Now, let's use the `scikit-learn` library implementation of the Multi-layer Perceptron to train the dataset with the chosen parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AuFqdmObSei"
      },
      "source": [
        "# training\n",
        "nn = MLPClassifier(random_state=SEED, max_iter=500, learning_rate_init=0.01, hidden_layer_sizes=(10, )).fit(D_train, y_train)\n",
        "\n",
        "# predicting\n",
        "y_predict_nn = nn.predict(D_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhApshb6ww1B"
      },
      "source": [
        "Printing performance metrics such as the *classifier score* and the *model selection method* that we defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnlZ0DcCbfpF"
      },
      "source": [
        "print()\n",
        "print(\"Classifier score:\\n\" , nn.score(D_train, y_train))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_train, y_predict_nn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYJvUhfYmf_F"
      },
      "source": [
        "## 2.4. SVM\n",
        "Initially, we had to choose between the several SVM implementation in the library. We began with a *Radial Basis Function (RBF)* kernel (using the `sklearn.svm.SVC` class) and, although it classified very well, the program took too long to finish compared to the previous methods, and it didn't compensate.\n",
        "\n",
        "Therefore, let's use the `GridSearchCV` to find the best candidates to parameters of a Linear Support Vector Machine:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIomWoMNSthl"
      },
      "source": [
        "tuned_parameters = [{'penalty':      ['l2'],\n",
        "                     'dual':         [False],\n",
        "                     'loss' :        ['hinge', 'squared_hinge'],\n",
        "                     'C':            [1, 10, 100, 1000],\n",
        "                     'tol':          [1e-3, 1e-4],\n",
        "                     'random_state': [SEED],\n",
        "                     'max_iter':     [5000]}]\n",
        "\n",
        "scores = ['precision', 'recall']\n",
        "\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
        "    print()\n",
        "\n",
        "    clf = GridSearchCV(\n",
        "        LinearSVC(), tuned_parameters, scoring='%s_macro' % score\n",
        "    )\n",
        "    clf.fit(D_train, y_train)\n",
        "\n",
        "    print(\"Best parameters set found on development set:\")\n",
        "    print()\n",
        "    print(clf.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on development set:\")\n",
        "    print()\n",
        "    means = clf.cv_results_['mean_test_score']\n",
        "    stds = clf.cv_results_['std_test_score']\n",
        "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A5aFpVQH0jx"
      },
      "source": [
        "Thus, we conclude that the best parameters in this model are:\n",
        "```\n",
        "C = 10,\n",
        "penalty = 'l2',\n",
        "loss = 'squared_hinge',\n",
        "dual = [False],\n",
        "tol = 1e-3,\n",
        "max_iter: 5000\n",
        "```\n",
        "(The selection of `max_iter` took into account how many iterations were necessary so that the model would converge, and it was adjusted manually)\n",
        "\n",
        "Using the `scikit-learn` library implementation of a Linear Support Vector Machine to train the dataset with the chosen parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yQkJwCHivG-"
      },
      "source": [
        "# training\n",
        "svm_clf = LinearSVC(random_state=SEED, max_iter=5000).fit(D_train, y_train)\n",
        "\n",
        "# predicting\n",
        "y_predict_svm = svm_clf.predict(D_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kLPgT_14e-"
      },
      "source": [
        "Printing performance metrics such as the *classifier score* and the *model selection method* that we defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhwy3-xQjZgW"
      },
      "source": [
        "print()\n",
        "print(\"Classifier score:\\n\" , svm_clf.score(D_train, y_train))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_train, y_predict_svm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK8O7ANFNUiQ"
      },
      "source": [
        "# 3. Choosing a final model\n",
        "Now, evaluating – on the validation set ($D_{val}$) – the three models selected in the previous step.  \n",
        " Let's compare the performance computed with respect to ($D_{val}$) with the ones obtained in the previous step (in $D_{train}$):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGY-7Xbd5BKK"
      },
      "source": [
        "## 3.1. Validating the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-t6LZk_nJSR"
      },
      "source": [
        "# predicting\n",
        "y_predict_val = clf.predict(D_val)\n",
        "\n",
        "# estimating metrics\n",
        "print()\n",
        "print('Classifier score:\\n', clf.score(D_val, y_val))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_val, y_predict_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIxm7yCN5Kj_"
      },
      "source": [
        "## 3.2. Validating the multi-layer perceptron model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY9N7Wmc0alV"
      },
      "source": [
        "# predicting\n",
        "y_predict_nn_val = nn.predict(D_val)\n",
        "\n",
        "# estimating metrics\n",
        "print()\n",
        "print('Classifier score:\\n', nn.score(D_val, y_val))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_val, y_predict_nn_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcpCRycp5RY7"
      },
      "source": [
        "## 3.3. Validating the linear SVM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzpmHayZ0bLb"
      },
      "source": [
        "# predicting\n",
        "y_predict_svm_val = svm_clf.predict(D_val)\n",
        "\n",
        "# estimating metrics\n",
        "print()\n",
        "print('Classifier score:\\n', svm_clf.score(D_val, y_val))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_val, y_predict_svm_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT13k20T44Y_"
      },
      "source": [
        "## 3.4. Comparing the three models\n",
        "The computed metrics on the validation set can be more clearly compared through the table:\n",
        "\n",
        "|                   | Score | MSE  | Accuracy |  \n",
        "|:-----------------:|:-----:|:----:|:--------:|  \n",
        "| Linear Regression | 0.91  | 1.56 | 0.91     |\n",
        "| Neural Networks   | 0.92  | 1.35 | 0.92     |\n",
        "| Linear SVM        | 0.90  | 1.74 | 0.90     |\n",
        "\n",
        "Thus, we can conclude that the **Neural Networks** have the highest score and accuracy, and also the lowest error; so that's our final chosen model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bveTKPbjNYUU"
      },
      "source": [
        "# 4. Error estimation\n",
        "Computing an estimate of the final model expected performance ($E_{out}$ and other metrics/analysis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHyBxVmu9OSv"
      },
      "source": [
        "## 4.1. Estimating metrics over $D_{test}$\n",
        "Printing $E_{out}$ and other metrics/analysis, using neural networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6VsfKGs2Uoc"
      },
      "source": [
        "# predicting\n",
        "y_predict_nn_test = nn.predict(X_test)\n",
        "\n",
        "# estimating metrics\n",
        "print()\n",
        "print('Classifier score:\\n', nn.score(X_test, y_test))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_test, y_predict_nn_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKeJbvcZ-hHM"
      },
      "source": [
        "Now, let's do a **Cross validation**, comparing the error outputs of training versus testing:\n",
        "\n",
        "|            | MSE  |\n",
        "|:----------:|:----:| \n",
        "| Training   | 1.20 | \n",
        "| Validating | 1.35 | \n",
        "| Testing    | 1.26 |\n",
        "\n",
        "<!-- TODO: COMENTAR SOBRE A CROSS VALIDATION -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AnyudlQ-vsN"
      },
      "source": [
        "## 4.2. Retraining the selected method on $D_{train} \\cup D_{val}$\n",
        "Now, let's retrain the neural network on $D_{train} \\cup D_{val}$ and verify if there is any significant\n",
        "difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VjPN5hZ9Brz"
      },
      "source": [
        "# training\n",
        "nn2 = MLPClassifier(random_state=SEED, max_iter=500, learning_rate_init=0.01, hidden_layer_sizes=(10, )).fit(X_train, y_train_padrao)\n",
        "\n",
        "# predicting\n",
        "y_predict_nn2 = nn2.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZkMgfMcAHGd"
      },
      "source": [
        "Printing performance metrics such as the *classifier score* and the *model selection method* that we defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AppPCOLM-d3O"
      },
      "source": [
        "# estimating metrics\n",
        "print()\n",
        "print('Classifier score:\\n', nn2.score(X_train, y_train_padrao))\n",
        "\n",
        "print()\n",
        "model_selection_metrics(y_train_padrao, y_predict_nn2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vScKJBpnAfeX"
      },
      "source": [
        "Computing an estimate of the final model expected performance ($E_{out}$ and other metrics/analysis):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNHrWwb7-SlA"
      },
      "source": [
        "y_predict_nn2_test = nn2.predict(X_test)\n",
        "print('Classifier score:\\n', nn2.score(X_test, y_test))\n",
        "\n",
        "model_selection_metrics(y_test, y_predict_nn2_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5r3e5HuCtPR"
      },
      "source": [
        "The performance metrics computed on $D_{train} \\cup D_{val}$ still hold the high score and accuracy, and also the low error. That means that the output error $E_{out}$ is low, and our choices of a model keep showing to be a precise fit for the dataset."
      ]
    }
  ]
}